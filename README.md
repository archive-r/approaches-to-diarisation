# Approaches to diarisation

_A testing repo to share code and thoughts on diarisation_

I am new to the field of NLP and struggling with diarisation. I have tried a number of state-of-the-art approaches. 

While several of them, namely those combining Whisper with Pyannote or Nemo, yield satisfactory results when it comes to the quality of the transcription and the aligment, speaker attribution is a different matter all together. 

Sometimes it works like a charm, sometimes it is a complete disaster, or anything in between. I wanted to understand why. During my tribulations, I came up with the approach showcased in the attached scripts (when going through my code keep in mind that I am chemist, not a developer).


SCRIPT 1: batch_diarize_stablets.py

0. If required, download the sample audio files from different sources and save them in a folder caller "diarisamples". 

1. Audio pre-processing: voice isolation with [demucs](https://github.com/facebookresearch/demucs), conversion to 16-bit 16MHz WAV with ffmpeg, normalisation with [pydub](https://github.com/jiaaro/pydub), and resaving with scipy (this step is required to prevent out of range errors from [pyannote](https://github.com/pyannote/pyannote-audio) segments timestamps).
   
2. Transcription and timestamp synchronisation with [Whisper](https://github.com/openai/whisper) via [stable_ts](https://github.com/jianfch/stable-ts).

3. Post-processing of the stable_ts output in order to have more consistent sentence splitting and creation of SRT subtitles in "diarisamples".


SCRIPT 2: batch_diarize_hdbscan.py

0. Takes the SRT and WAV files generated by the previous script from "diarisamples" as input.

1. Computes the embeddings for each segment with a [TitaNet](https://huggingface.co/nvidia/speakerverification_en_titanet_large) model. Alternatively, [ECAPA-TDNN](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb) can be used, but it yields somehow less accurate results. Note that in the first case you will need [NeMo](https://github.com/NVIDIA/NeMo) which is not compatible with the most recent Python.

2. Computes all versus all consine distance matrices with scipy (this code chunk in particular is so ugly and inefficient that would make van Rossum cry).

3. Dimensionality reduction of the distance matrices with [UMAP](https://github.com/lmcinnes/umap).

4. Clustering of the UMAP embeddings with [HDBSCAN](https://github.com/scikit-learn-contrib/hdbscan).

5. Interactive 3D plots with [plotly](https://github.com/plotly/plotly.py) and saving of the diarised SRT files. 


My procedure (work in progress) yields better results than anything else I had tried at the time of this writing. Please, feel free to fork and contribute.

The 22 samples used contain 1-3 speakers and UMAP and HDBSCAN parametres have been calibrated accordingly, but, in principle, this procedure should support any unknown number of speakers. Heuristics or ML may be required in order to guess the ideal parameters for each sample. 

